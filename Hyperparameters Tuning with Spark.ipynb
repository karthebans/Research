{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Effortless Hyperparameters Tuning with Apache Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "\u001b[K     |████████████████████████████████| 498 kB 65.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20.3; python_version < \"3.10\"\n",
      "  Downloading numpy-1.23.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 64.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /config/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /config/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/config/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed numpy-1.23.4 pandas-1.5.1 pytz-2022.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /config/.local/lib/python3.8/site-packages (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2 MB 22.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=1.0.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 42.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 33.8 MB 65.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /config/.local/lib/python3.8/site-packages (from scikit-learn) (1.23.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: joblib, scipy, threadpoolctl, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.1.3 scipy-1.9.3 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.1-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 193.6 MB 167 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /config/.local/lib/python3.8/site-packages (from xgboost) (1.23.4)\n",
      "Requirement already satisfied: scipy in /config/.local/lib/python3.8/site-packages (from xgboost) (1.9.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting interruptingcow\n",
      "  Downloading interruptingcow-0.8.tar.gz (5.0 kB)\n",
      "Building wheels for collected packages: interruptingcow\n",
      "  Building wheel for interruptingcow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for interruptingcow: filename=interruptingcow-0.8-py3-none-any.whl size=5804 sha256=1a477818fd15c4fbb5cbfbb6cffb708d7111b61bd9d38d88376b7942785b22a2\n",
      "  Stored in directory: /config/.cache/pip/wheels/ed/43/cc/2a5511c6cabcee2e975227e93fb05c708e406844b0ff24ea49\n",
      "Successfully built interruptingcow\n",
      "Installing collected packages: interruptingcow\n",
      "Successfully installed interruptingcow-0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install interruptingcow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting preprocessing\n",
      "  Downloading preprocessing-0.1.13-py3-none-any.whl (349 kB)\n",
      "\u001b[K     |████████████████████████████████| 349 kB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinx-rtd-theme==0.2.4\n",
      "  Downloading sphinx_rtd_theme-0.2.4-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 55.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk==3.2.4\n",
      "  Downloading nltk-3.2.4.tar.gz (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 66.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /config/.local/lib/python3.8/site-packages (from nltk==3.2.4->preprocessing) (1.16.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.2.4-py3-none-any.whl size=1367704 sha256=66a204aa505366b2e0acecea91726e6353e78e6c6adc7971f12925896741c0e3\n",
      "  Stored in directory: /config/.cache/pip/wheels/d9/37/86/b5270b826e4b542bd6791005300c9d3864059901c7efc03545\n",
      "Successfully built nltk\n",
      "Installing collected packages: sphinx-rtd-theme, nltk, preprocessing\n",
      "Successfully installed nltk-3.2.4 preprocessing-0.1.13 sphinx-rtd-theme-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.3.2-cp38-cp38-manylinux2014_x86_64.whl (518 kB)\n",
      "\u001b[K     |████████████████████████████████| 518 kB 21.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n",
      "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 60.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.2.1 pymongo-4.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblibspark\n",
      "  Downloading joblibspark-0.5.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in /config/.local/lib/python3.8/site-packages (from joblibspark) (1.2.0)\n",
      "Installing collected packages: joblibspark\n",
      "Successfully installed joblibspark-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install joblibspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://gist.githubusercontent.com/aialenti/22ee46bb7cfc714286c881cc556cb9f0/raw/395a57e5462a6d352319dda4e9cad11a5b07ab35/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    # Read input data\n",
    "    url = 'https://gist.githubusercontent.com/aialenti/22ee46bb7cfc714286c881cc556cb9f0/raw/395a57e5462a6d352319dda4e9cad11a5b07ab35/file.csv'\n",
    "    train = pd.read_csv(url)\n",
    "    categorical = [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]\n",
    "\n",
    "    #   Convert categorical data\n",
    "    for c in categorical:\n",
    "        group_by = train.groupby(by=c)[\"y\"].mean().reset_index().rename(columns={\"y\": \"{}_converted\".format(c)})\n",
    "        train = pd.merge(train, group_by, how='inner', on=c)\n",
    "\n",
    "    train = train.drop(categorical, axis=1)\n",
    "\n",
    "    #   Drop the ID column\n",
    "    X = train.drop(\"ID\", axis=1).drop(\"y\", axis=1)\n",
    "    y = train[\"y\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   X10  X11  X12  X13  X14  X15  X16  X17  X18  X19  ...  X384  X385  \\\n",
       " 0    0    0    0    1    0    0    0    0    1    0  ...     0     0   \n",
       " 1    0    0    0    0    0    0    0    0    1    0  ...     0     0   \n",
       " 2    0    0    0    0    0    0    0    1    0    0  ...     0     0   \n",
       " 3    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       " 4    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       " \n",
       "    X0_converted  X1_converted  X2_converted  X3_converted  X4_converted  \\\n",
       " 0        109.67       104.415        130.81        130.81        90.848   \n",
       " 1        109.67        84.575         88.53         88.53        90.848   \n",
       " 2         78.30        76.260         78.30         76.26        90.848   \n",
       " 3         78.30       104.415         78.30         79.32        90.848   \n",
       " 4         78.30        84.575         78.30         79.32        90.848   \n",
       " \n",
       "    X5_converted  X6_converted  X8_converted  \n",
       " 0        130.81       103.535        109.67  \n",
       " 1         88.53        84.575        109.67  \n",
       " 2         78.44       103.535         76.26  \n",
       " 3         78.02        78.020         78.02  \n",
       " 4         78.44        84.575         80.62  \n",
       " \n",
       " [5 rows x 376 columns],\n",
       " 0    130.81\n",
       " 1     88.53\n",
       " 2     76.26\n",
       " 3     78.02\n",
       " 4     80.62\n",
       " Name: y, dtype: float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [float(v) for v in np.arange(0.01, 0.25, 0.01)],\n",
    "              'colsample_bytree': [float(v) for v in np.arange(0.8, 1.01, 0.1)],\n",
    "              'subsample': [float(v) for v in np.arange(0.5, 1.01, 0.1)],\n",
    "              'n_estimators': [int(v) for v in np.arange(100, 3000, 100)],\n",
    "              'reg_alpha': [float(v) for v in np.arange(0.01, 0.5, 0.05)],\n",
    "              'max_depth': [int(v) for v in np.arange(3, 14, 1)],\n",
    "              'gamma': [int(v) for v in np.arange(0, 10, 2)]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_iterable():\n",
    "    param_grid = {'learning_rate': [float(v) for v in np.arange(0.01, 0.25, 0.01)],\n",
    "                  'colsample_bytree': [float(v) for v in np.arange(0.8, 1.01, 0.1)],\n",
    "                  'subsample': [float(v) for v in np.arange(0.5, 1.01, 0.1)],\n",
    "                  'n_estimators': [int(v) for v in np.arange(100, 3000, 100)],\n",
    "                  'reg_alpha': [float(v) for v in np.arange(0.01, 0.5, 0.05)],\n",
    "                  'max_depth': [int(v) for v in np.arange(3, 14, 1)],\n",
    "                  'gamma': [int(v) for v in np.arange(0, 10, 2)]\n",
    "                  }\n",
    "    grid_iter = []\n",
    "    length = 1\n",
    "    for k in param_grid:\n",
    "        grid_iter.append(param_grid[k])\n",
    "        length *= len(param_grid[k])\n",
    "\n",
    "    return itertools.product(*grid_iter), list(param_grid.keys()), length-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_r2_score(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'r2', r2_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itertools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 80\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tests)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboosting_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [32], line 29\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(timeout_seconds, cv_splits, boosting_rounds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#   Execute until timeout occurs\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout(timeout_seconds, exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeError\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#   Get the grid\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     grid_iter, keys, length \u001b[38;5;241m=\u001b[39m \u001b[43mget_grid_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m#   For every element of the grid\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df_grid \u001b[38;5;129;01min\u001b[39;00m grid_iter:\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;66;03m#   Prepare a list to collect the scores\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 16\u001b[0m, in \u001b[0;36mget_grid_iterable\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     grid_iter\u001b[38;5;241m.\u001b[39mappend(param_grid[k])\n\u001b[1;32m     14\u001b[0m     length \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(param_grid[k])\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mitertools\u001b[49m\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mgrid_iter), \u001b[38;5;28mlist\u001b[39m(param_grid\u001b[38;5;241m.\u001b[39mkeys()), length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itertools' is not defined"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from interruptingcow import timeout\n",
    "from sklearn.model_selection import KFold  # import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import json\n",
    "# from preprocessing import preprocessing, xgb_r2_score # The preprocessing and the r2 evaluation\n",
    "# from generate_grid import get_grid_iterable # The grid\n",
    "\n",
    "\n",
    "def grid_search(timeout_seconds, cv_splits, boosting_rounds):\n",
    "    #   Read input data\n",
    "    X, y = preprocessing()\n",
    "\n",
    "    #   Create dataframe to collect the results\n",
    "    tests_columns = [\"test_nr\", \"cv_mean\", \"cv_min\", \"cv_max\", \"cv_median\", \"params\"]\n",
    "    test_id = 0\n",
    "    tests = pd.DataFrame(columns=tests_columns)\n",
    "\n",
    "    #   Cross validation number of splits\n",
    "    kf = KFold(n_splits=cv_splits)\n",
    "\n",
    "    #   Execute until timeout occurs\n",
    "    with timeout(timeout_seconds, exception=RuntimeError):\n",
    "\n",
    "        #   Get the grid\n",
    "        grid_iter, keys, length = get_grid_iterable()\n",
    "        try:\n",
    "\n",
    "            #   For every element of the grid\n",
    "            for df_grid in grid_iter:\n",
    "                #   Prepare a list to collect the scores\n",
    "                score = []\n",
    "                params = dict(zip(keys, df_grid))\n",
    "\n",
    "                #   The objective function\n",
    "                params[\"objective\"] = \"reg:squarederror\"\n",
    "\n",
    "                #   For each fold, train XGBoost and spit out the results\n",
    "                for train_index, test_index in kf.split(X.values):\n",
    "\n",
    "                    #   Get X train and X test\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "                    #   Get y train and y test\n",
    "                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                    #   Convert into DMatrix\n",
    "                    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "                    d_valid = xgb.DMatrix(X_test, label=y_test)\n",
    "                    d_test = xgb.DMatrix(X_test)\n",
    "                    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "                    #   Create the classifier using the current grid params. Apply early stopping of 50 rounds\n",
    "                    clf = xgb.train(params, d_train, boosting_rounds, watchlist, early_stopping_rounds=50,\n",
    "                                    feval=xgb_r2_score, maximize=True, verbose_eval=10)\n",
    "                    y_hat = clf.predict(d_test)\n",
    "\n",
    "                    #   Append Scores on the fold kept out\n",
    "                    score.append(r2_score(y_test, y_hat))\n",
    "\n",
    "                #   Store the result into a dataframe\n",
    "                score_df = pd.DataFrame(columns=tests_columns, data=[\n",
    "                    [test_id, np.mean(score), np.min(score), np.max(score), np.median(score),\n",
    "                     json.dumps(dict(zip(keys, [str(g) for g in df_grid])))]])\n",
    "                test_id += 1\n",
    "                tests = pd.concat([tests, score_df])\n",
    "        except RuntimeError:\n",
    "            #   When timeout occurs an exception is raised and the main cycle is broken\n",
    "            pass\n",
    "\n",
    "    #   Spit out the results\n",
    "    tests.to_csv(\"grid-search.csv\", index=False)\n",
    "    print(tests)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search(timeout_seconds=3600, cv_splits=4, boosting_rounds=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interruptingcow import timeout\n",
    "from sklearn.model_selection import KFold\n",
    "import pymongo\n",
    "from joblibspark import register_spark\n",
    "from joblib import Parallel, delayed\n",
    "from pprint import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_spark(timeout_seconds, cv_splits, boosting_rounds):\n",
    "    #   Read and preprocess input data\n",
    "    X, y = preprocessing()\n",
    "\n",
    "    #   !!! First step to use Joblib Spark\n",
    "    register_spark()  # register spark backend\n",
    "\n",
    "    kf = KFold(n_splits=cv_splits)\n",
    "\n",
    "    #   Init MongoDB\n",
    "    client = pymongo.MongoClient(MONGO_IP, 27017)\n",
    "    db = client[MONGO_DATABASE]\n",
    "    collection = db[MONGO_COLLECTION]\n",
    "    collection.drop()\n",
    "\n",
    "    #   InterruptingCow will kill everything after the timeout\n",
    "    with timeout(timeout_seconds, exception=RuntimeError):\n",
    "        try:\n",
    "            #   !!! The actual Joblib magic\n",
    "            Parallel(backend=\"spark\", n_jobs=NODES)(\n",
    "                delayed(evaluate)(X, y, kf, boosting_rounds) for p in range(0, NODES))\n",
    "        except RuntimeError:\n",
    "            print(\"Runtime\")\n",
    "    db = client[MONGO_DATABASE]\n",
    "    collection = db[MONGO_COLLECTION]\n",
    "\n",
    "    cursor = collection.find({})\n",
    "    for document in cursor:\n",
    "        pprint(document)\n",
    "    client.close()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "import pymongo\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   \"evaluate\" is the function that is executed on the single Spark executor\n",
    "def evaluate(X, y, kf, boosting_rounds):\n",
    "    # We need an inner function because we want to use multithreading on each node\n",
    "    def evaluate_inner(X, y, kf, boosting_rounds):\n",
    "        score = []\n",
    "        # !!! Pick a random point of the serach space - Conceptually is like running NODES distinct Random Search\n",
    "        params = generate_random_configuration()\n",
    "        params[\"objective\"] = \"reg:squarederror\"\n",
    "        print(params)\n",
    "        for train_index, test_index in kf.split(X.values):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "            d_valid = xgb.DMatrix(X_test, label=y_test)\n",
    "            d_test = xgb.DMatrix(X_test)\n",
    "            watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "            clf = xgb.train(params, d_train, boosting_rounds, watchlist, early_stopping_rounds=50,\n",
    "                            feval=xgb_r2_score, maximize=True, verbose_eval=25)\n",
    "            y_hat = clf.predict(d_test)\n",
    "            score.append(r2_score(y_test, y_hat))\n",
    "\n",
    "        output_dict = {\n",
    "            'min_score': np.min(score),\n",
    "            'max_score': np.max(score),\n",
    "            'average_score': np.mean(score),\n",
    "            'median_score': np.median(score),\n",
    "            'params': params\n",
    "        }\n",
    "\n",
    "        client = pymongo.MongoClient(MONGO_IP, 27017)\n",
    "        db = client[MONGO_DATABASE]\n",
    "        collection = db[MONGO_COLLECTION]\n",
    "        collection.insert_one(output_dict)\n",
    "        client.close()\n",
    "        return score\n",
    "\n",
    "    # Same concept as seen in the Spark parallelization, but now we are using plain Joblib to parallelize\n",
    "    # the instances in the chunk. In this case we are specifying how many cores we want to use (CORES variable)\n",
    "    Parallel()(delayed(evaluate_inner)(X, y, kf, boosting_rounds) for i in range(0, 100000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeout_seconds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m random_search_spark(\u001b[43mtimeout_seconds\u001b[49m, cv_splits, boosting_rounds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeout_seconds' is not defined"
     ]
    }
   ],
   "source": [
    "random_search_spark(timeout_seconds, cv_splits, boosting_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/10 20:53:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e620202c169b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f99fd100070>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = \"https://raw.githubusercontent.com/muttinenisairohith/Encoding-Categorical-Data/6e7bec3b9cbdc25da1055472c837ef8a10f569ed/data/car_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40.csv.\n: java.lang.UnsupportedOperationException\n\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:94)\n\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:565)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_pyspark \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_pyspark\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:410\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m     path \u001b[39m=\u001b[39m [path]\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    411\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    412\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.csv.\n: java.lang.UnsupportedOperationException\n\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:94)\n\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:565)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv(url_data,inferSchema=True, header=True)\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
